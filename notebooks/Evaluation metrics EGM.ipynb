{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bb30fc-a7e7-4ff1-8153-534c9eb37fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/nanopyx/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for _njit_cubic\n",
      "  warnings.warn(f\"Numba is not installed. Using pure python for {func.__name__}\")\n",
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/nanopyx/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for _njit_interpolate\n",
      "  warnings.warn(f\"Numba is not installed. Using pure python for {func.__name__}\")\n",
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/nanopyx/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for njit_shift_magnify\n",
      "  warnings.warn(f\"Numba is not installed. Using pure python for {func.__name__}\")\n",
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/nanopyx/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for njit_shift_scale_rotate\n",
      "  warnings.warn(f\"Numba is not installed. Using pure python for {func.__name__}\")\n",
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/nanopyx/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for _njit_mandelbrot\n",
      "  warnings.warn(f\"Numba is not installed. Using pure python for {func.__name__}\")\n",
      "/Users/esti/mambaforge/envs/microscopy/lib/python3.10/site-packages/nanopyx/__njit__.py:15: UserWarning: Numba is not installed. Using pure python for njit_mandelbrot\n",
      "  warnings.warn(f\"Numba is not installed. Using pure python for {func.__name__}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/esti/Documents/PROYECTOS/TFM-IVANHIDALGO/MicroscoPy/\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage import metrics as skimage_metrics\n",
    "from skimage.util import img_as_ubyte\n",
    "import copy\n",
    "from skimage import io\n",
    "\n",
    "# LPIPS metrics with AlexNet and VGG\n",
    "import lpips\n",
    "lpips_alex = lpips.LPIPS(net=\"alex\", version=\"0.1\")\n",
    "lpips_vgg = lpips.LPIPS(net=\"vgg\", version=\"0.1\")\n",
    "\n",
    "# Nanopyx metrics: Error map (RSE and RSP) and decorrelation analysis \n",
    "from nanopyx.core.transform.error_map import ErrorMap\n",
    "from nanopyx.core.analysis.decorr import DecorrAnalysis\n",
    "\n",
    "# ILNIQE (in a local file)\n",
    "import sys\n",
    "sys.path.append(\"/Users/esti/Documents/PROYECTOS/TFM-IVANHIDALGO/MicroscoPy\")\n",
    "sys.path.append(\"/Users/esti/Documents/PROYECTOS/TFM-IVANHIDALGO/MicroscoPy/microscopy/\")\n",
    "from ILNIQE import calculate_ilniqe\n",
    "#from IL-NIQUE import calculate_ilniqe\n",
    "\n",
    "\n",
    "def merge_lists(list1, list2):\n",
    "    return list1 + list2\n",
    "\n",
    "def merge_dict(dict1, dict2):\n",
    "    # Merge dictionaries with complex data structures\n",
    "    merged_dict = {}\n",
    "    for key in set(dict1) | set(dict2):\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(dict2[key], list):\n",
    "                merged_dict[key] = merge_lists(dict1[key], dict2[key])\n",
    "            elif isinstance(dict1[key], dict) and isinstance(dict2[key], dict):\n",
    "                merged_dict[key] = merge_nested_dicts(copy.deepcopy(dict1[key]), dict2[key])\n",
    "            else:\n",
    "                merged_dict[key] = dict2[key]\n",
    "        elif key in dict1:\n",
    "            merged_dict[key] = copy.deepcopy(dict1[key])\n",
    "        else:\n",
    "            merged_dict[key] = copy.deepcopy(dict2[key])\n",
    "    \n",
    "    # Output the merged dictionary\n",
    "    return(merged_dict)\n",
    "def unint16touint8(im):\n",
    "    im = im.astype(np.float32)\n",
    "    im = (im*255)/((2^16)-1)\n",
    "    return im.astype(np.uint8)\n",
    "\n",
    "def evaluate_dataset(gt_dir, predict_dir, input_dir):\n",
    "    im_list = os.listdir(gt_dir)\n",
    "    for i in tqdm(range(len(im_list))):\n",
    "        \n",
    "        gt_image = io.imread(os.path.join(gt_dir, im_list[i]))\n",
    "        gt_image = [min_max_normalization(gt_image)]\n",
    "        \n",
    "        predicted_image = io.imread(os.path.join(predict_dir, im_list[i]))\n",
    "        predicted_image = [min_max_normalization(predicted_image)]\n",
    "        \n",
    "        input_image = io.imread(os.path.join(input_dir, im_list[i]))\n",
    "        input_image = [min_max_normalization(input_image)]\n",
    "        \n",
    "        im_metrics = obtain_metrics(gt_image, predicted_image, input_image)\n",
    "        im_metrics[\"files\"] = [im_list[i]]\n",
    "        if i == 0:\n",
    "            metrics_dict = im_metrics\n",
    "        else:\n",
    "            metrics_dict = merge_dict(metrics_dict, im_metrics)\n",
    "\n",
    "    return metrics_dict\n",
    "#\n",
    "# Functions that define normalization tecniques \n",
    "# TODO: to import from utils\n",
    "def min_max_normalization(data, desired_accuracy=np.float32):\n",
    "    \"\"\"\n",
    "    Normalize the given data using min-max normalization.\n",
    "\n",
    "    Parameters:\n",
    "        data (ndarray): The data to be normalized.\n",
    "        desired_accuracy (type): The desired accuracy of the normalized data. Defaults to np.float32.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The normalized data.\n",
    "    \"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min() + 1e-10).astype(\n",
    "        desired_accuracy\n",
    "    )\n",
    "    \n",
    "\n",
    "def obtain_metrics(gt_image_list, predicted_image_list, wf_image_list, test_metric_indexes=[]):\n",
    "    \"\"\"\n",
    "    Calculate various metrics for evaluating the performance of an image prediction model.\n",
    "\n",
    "    Args:\n",
    "        gt_image_list (List[np.ndarray]): A list of ground truth images.\n",
    "        predicted_image_list (List[np.ndarray]): A list of predicted images with the same name as the ground truth images.\n",
    "        wf_image_list (List[np.ndarray]): A list of input images with the same name as the ground truth and predicted images.\n",
    "        test_metric_indexes (List[int]): A list of indexes to calculate additional metrics.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing different metrics as keys and their corresponding values as lists.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the minimum value of the wavefront image is greater than 0 or the maximum value is less than 0.\n",
    "\n",
    "    Note:\n",
    "        This function uses various image metrics including MSE, SSIM, PSNR, GT RSE, GT RSP, Pred RSE, Pred RSP, and Decorrelation.\n",
    "        It also calculates metrics using the LPIPS (Learned Perceptual Image Patch Similarity) model, ILNIQE (Image Lab Non-Reference Image Quality Evaluation), and other metrics.\n",
    "        The calculated metrics are stored in a dictionary with the metric names as keys and lists of values as their corresponding values.\n",
    "    \"\"\"\n",
    "    metrics_dict = {\n",
    "        \"ssim\": [],\n",
    "        \"psnr\": [],\n",
    "        \"mse\": [],\n",
    "        \"alex\": [],\n",
    "        \"vgg\": [],\n",
    "        \"ilniqe\": [],\n",
    "        \"fsim\": [],\n",
    "        \"gmsd\": [],\n",
    "        \"vsi\": [],\n",
    "        \"haarpsi\": [],\n",
    "        \"mdsi\": [],\n",
    "        \"pieapp\": [],\n",
    "        \"dists\": [],\n",
    "        \"brisqe\": [],\n",
    "        \"fid\": [],\n",
    "        \"gt_rse\":[],\n",
    "        \"gt_rsp\":[],\n",
    "        \"pred_rse\":[],\n",
    "        \"pred_rsp\":[],\n",
    "        \"decor\":[]\n",
    "    }\n",
    "\n",
    "    test_data_length = len(gt_image_list)\n",
    "    for i in tqdm(range(test_data_length)):\n",
    "        \n",
    "        # Load the widefield image, ground truth image, and predicted image\n",
    "        gt_image = np.squeeze(gt_image_list[i]) # gt_image_list[i][:, :, 0]\n",
    "        predicted_image = np.squeeze(predicted_image_list[i]) # predicted_image_list[i][:, :, 0]\n",
    "        wf_image = np.squeeze(wf_image_list[i]) # wf_image_list[i][:, :, 0]\n",
    "\n",
    "\n",
    "        # Print info about the images\n",
    "        print(\n",
    "            f\"gt_image shape: {gt_image.shape} - intensity range: {gt_image.min()} {gt_image.max()} - data type {gt_image.dtype}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"predicted_image shape: {predicted_image.shape} - intensity range: {predicted_image.min()} {predicted_image.max()} - data type {predicted_image.dtype}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"wf_image shape: {wf_image.shape} - intensity range: {wf_image.min()} {wf_image.max()} - data type {wf_image.dtype}\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Convert the Numpy images into Pytorch tensors\n",
    "        # Pass the images into Pytorch format (1, 1, X, X)\n",
    "        gt_image_piq = np.expand_dims(gt_image, axis=[0, 1])\n",
    "        predicted_image_piq = np.expand_dims(predicted_image, axis=[0, 1])\n",
    "        \n",
    "        # Pytorch does not support uint16\n",
    "        if gt_image_piq.dtype == np.uint16:\n",
    "            gt_image_piq = unint16touint8(gt_image_piq)\n",
    "        if predicted_image_piq.dtype == np.uint16:\n",
    "            predicted_image_piq = unint16touint8(predicted_image_piq)\n",
    "            \n",
    "        # Convert the images into Pytorch tensors\n",
    "        gt_image_piq = torch.from_numpy(gt_image_piq)\n",
    "        predicted_image_piq = torch.from_numpy(predicted_image_piq)\n",
    "\n",
    "        \n",
    "        # Assert that there are no negative values\n",
    "        assert wf_image.min() >= 0. and wf_image.max() >= 0.\n",
    "\n",
    "        # In case all the predicted values are equal (all zeros for example)\n",
    "        all_equals = np.all(predicted_image==np.ravel(predicted_image)[0])\n",
    "\n",
    "    \n",
    "        #####################################\n",
    "        #\n",
    "        # Calculate the skimage metrics\n",
    "        print(\"Calculating standard pixel based metrics\")\n",
    "        print(\"__________________________________________\")\n",
    "        metrics_dict[\"mse\"].append(\n",
    "            skimage_metrics.mean_squared_error(gt_image, predicted_image)\n",
    "        )\n",
    "\n",
    "        metrics_dict[\"ssim\"].append(\n",
    "            skimage_metrics.structural_similarity(\n",
    "                predicted_image, gt_image, data_range=1.0\n",
    "            )\n",
    "        )\n",
    "        metrics_dict[\"psnr\"].append(\n",
    "            skimage_metrics.peak_signal_noise_ratio(gt_image, predicted_image)\n",
    "        )\n",
    "\n",
    "        #\n",
    "        #####################################\n",
    "\n",
    "        #####################################\n",
    "        #\n",
    "        # Calculate the LPIPS metrics\n",
    "        print(\"Calculating LIPIPS metric\")\n",
    "        print(\"______________________________\")\n",
    "        metrics_dict[\"alex\"].append(\n",
    "                np.squeeze(\n",
    "                    lpips_alex(gt_image_piq.float(), predicted_image_piq.float())\n",
    "                    .detach()\n",
    "                    .numpy()\n",
    "                )\n",
    "            )\n",
    "        metrics_dict[\"vgg\"].append(\n",
    "            np.squeeze(\n",
    "                lpips_vgg(gt_image_piq.float(), predicted_image_piq.float())\n",
    "                .detach()\n",
    "                .numpy()\n",
    "            ))\n",
    "        \n",
    "        \n",
    "        #####################################\n",
    "\n",
    "        #####################################\n",
    "        #\n",
    "        # Calculate the Nanopyx metrics\n",
    "        print(\"Calculating Nanopyx metrics\")\n",
    "        print(\"______________________________\")\n",
    "        error_map = ErrorMap()\n",
    "        error_map.optimise(wf_image, gt_image)\n",
    "        metrics_dict[\"gt_rse\"].append(\n",
    "            error_map.getRSE()\n",
    "        )\n",
    "        metrics_dict[\"gt_rsp\"].append(\n",
    "            error_map.getRSP()\n",
    "        )\n",
    "\n",
    "        if not all_equals:\n",
    "            error_map = ErrorMap()\n",
    "            error_map.optimise(wf_image, predicted_image)\n",
    "            metrics_dict[\"pred_rse\"].append(\n",
    "                error_map.getRSE()\n",
    "            )\n",
    "            metrics_dict[\"pred_rsp\"].append(\n",
    "                error_map.getRSP()\n",
    "            )\n",
    "        else: \n",
    "            metrics_dict[\"pred_rse\"].append(np.nan)\n",
    "            metrics_dict[\"pred_rsp\"].append(np.nan)\n",
    "\n",
    "        if not all_equals:\n",
    "            decorr_calculator_raw = DecorrAnalysis()\n",
    "            decorr_calculator_raw.run_analysis(predicted_image)\n",
    "            metrics_dict[\"decor\"].append(\n",
    "                decorr_calculator_raw.resolution\n",
    "            )\n",
    "        else: \n",
    "            metrics_dict[\"decor\"].append(np.nan)\n",
    "\n",
    "        #####################################\n",
    "\n",
    "        #####################################\n",
    "        #\n",
    "        # Calculate the ILNIQE\n",
    "        print(\"Calculating IL-NIQUE metric\")\n",
    "        print(\"______________________________\")\n",
    "        # Temporally commented to avoid long evaluation times (83 seconds for each image)\n",
    "        if not all_equals:\n",
    "            metrics_dict['ilniqe'].append(calculate_ilniqe(img_as_ubyte(predicted_image), 0,\n",
    "                                            input_order='HW', resize=True, version='python'))\n",
    "        else: \n",
    "            metrics_dict['ilniqe'].append(np.nan)\n",
    "\n",
    "        \n",
    "        #####################################\n",
    "\n",
    "        \n",
    "        if i in test_metric_indexes:\n",
    "            # In case you want to calculate in specific images (a reduced number to avoid time issues)\n",
    "            pass\n",
    "        \n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a654fb3-8836-40c3-ac20-d75ffca4c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                           | 0/12 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_image shape: (2048, 2048) - intensity range: 0.0 1.0 - data type float32\n",
      "predicted_image shape: (2048, 2048) - intensity range: 0.0 1.0 - data type float32\n",
      "wf_image shape: (256, 256) - intensity range: 0.0 1.0 - data type float32\n",
      "Calculating standard pixel based metrics\n",
      "__________________________________________\n",
      "Calculating LIPIPS metric\n",
      "______________________________\n",
      "Calculating Nanopyx metrics\n",
      "______________________________\n",
      "Querying the Agent...\n",
      "Agent: ShiftMagnify_CR using OpenCL_Apple M1 ran in 0.06551862499327399 seconds\n",
      "Querying the Agent...\n",
      "Agent: ShiftMagnify_CR using OpenCL_Apple M1 ran in 0.017144000012194738 seconds\n",
      "Calculating IL-NIQUE metric\n",
      "______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:54<00:00, 114.47s/it]\u001b[A\n",
      "  8%|████████████▏                                                                                                                                     | 1/12 [01:54<20:59, 114.51s/it]\n",
      "  0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_image shape: (2048, 2048) - intensity range: 0.0 1.0 - data type float32\n",
      "predicted_image shape: (2048, 2048) - intensity range: 0.0 1.0 - data type float32\n",
      "wf_image shape: (256, 256) - intensity range: 0.0 1.0 - data type float32\n",
      "Calculating standard pixel based metrics\n",
      "__________________________________________\n",
      "Calculating LIPIPS metric\n",
      "______________________________\n",
      "Calculating Nanopyx metrics\n",
      "______________________________\n",
      "Querying the Agent...\n",
      "Agent: ShiftMagnify_CR using OpenCL_Apple M1 ran in 0.018396249986835755 seconds\n",
      "Querying the Agent...\n",
      "Agent: ShiftMagnify_CR using OpenCL_Apple M1 ran in 0.019851499993819743 seconds\n",
      "Calculating IL-NIQUE metric\n",
      "______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:10<00:00, 130.21s/it]\u001b[A\n",
      " 17%|████████████████████████▎                                                                                                                         | 2/12 [04:04<20:37, 123.78s/it]\n",
      "  0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_image shape: (2048, 2048) - intensity range: 0.0 1.0 - data type float32\n",
      "predicted_image shape: (2048, 2048) - intensity range: 0.0 1.0 - data type float32\n",
      "wf_image shape: (256, 256) - intensity range: 0.0 1.0 - data type float32\n",
      "Calculating standard pixel based metrics\n",
      "__________________________________________\n",
      "Calculating LIPIPS metric\n",
      "______________________________\n",
      "Calculating Nanopyx metrics\n",
      "______________________________\n",
      "Querying the Agent...\n",
      "Agent: ShiftMagnify_CR using OpenCL_Apple M1 ran in 0.029432915995130315 seconds\n",
      "Querying the Agent...\n",
      "Agent: ShiftMagnify_CR using OpenCL_Apple M1 ran in 0.016967749994364567 seconds\n",
      "Calculating IL-NIQUE metric\n",
      "______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:43<00:00, 103.27s/it]\u001b[A\n",
      " 25%|████████████████████████████████████▌                                                                                                             | 3/12 [05:48<17:09, 114.44s/it]\n",
      "  0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_image shape: (4000, 4000) - intensity range: 0.0 1.0 - data type float32\n",
      "predicted_image shape: (4000, 4000) - intensity range: 0.0 1.0 - data type float32\n",
      "wf_image shape: (500, 500) - intensity range: 0.0 1.0 - data type float32\n",
      "Calculating standard pixel based metrics\n",
      "__________________________________________\n",
      "Calculating LIPIPS metric\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "gt_dir = \"/Users/esti/Documents/PROYECTOS/TFM-IVANHIDALGO/toy_data/MT-SMLM-registered/gt\"\n",
    "input_dir = \"/Users/esti/Documents/PROYECTOS/TFM-IVANHIDALGO/toy_data/MT-SMLM-registered/wf/\"\n",
    "predict_dir = \"/Users/esti/Documents/PROYECTOS/TFM-IVANHIDALGO/toy_data/MT-SMLM-registered/rcan/epc200_btch4_lr0.0001_optim-adam_lrsched-ReduceOnPlateau_seed666_1/prediction/\"\n",
    "evaluate_dataset(gt_dir, predict_dir, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7b492-29c8-4dd4-8e37-e20b4447715c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
